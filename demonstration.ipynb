{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Agnostic Topic Modelling (DATM)\n",
    "### This is an exploratory notebook to explore the topicmodelling tool that I have created. See the submitted paper for details.\n",
    "\n",
    "Please follow the steps below. Most of them are optional since the data has already been cleaned and transformed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import this if you cannot access the stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# !pip install --yes --prefix {sys.prefix} nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the generic import statements.\n",
    "#-------------------------------------------------------------------------\n",
    "#Classic Imports\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from  sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import ast\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pathlib\n",
    "\n",
    "from biterm.btm import oBTM\n",
    "from biterm.cbtm import oBTM as c_oBTM\n",
    "from biterm.utility import vec_to_biterms, topic_summuary\n",
    "import pickle\n",
    "#--------------------------------------------------------------\n",
    "#Local Imports\n",
    "from src.data import DataPreprocessing as dp # General data processing functions\n",
    "from src.sompy import sompy as sompy #SOM module\n",
    "from src.data import PreprocessAirlineData as PAD #Specific data processing for the airline data\n",
    "from src.models import Modelling as mdl #Contains functions for the data trasformation and topic models\n",
    "from src.evaluations import evaluation as ev #evaluation modules\n",
    "import imp\n",
    "# imp.reload(ev)\n",
    "from src.generalFunctions import functions as fs # general functions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preprocessing\n",
    "\n",
    "Please note that the Data has already been preproccessed and cleaned.\n",
    "The following cells lets us replicate all the data preprocessing steps that we have already developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell specifies where all data must be stored and retreived from\n",
    "# dataFolder = str(pathlib.Path().absolute().parents[1]) +\"/data\" #points to the data folder\n",
    "dataFolder = \"./data\" #points to the data folder\n",
    "airlineRawData = dataFolder+\"/airlineDataset/raw\" #points to the raw data folder\n",
    "airlineProcessedData = dataFolder+\"/airlineDataset/processed\" #points to the processed data folder\n",
    "airlineResults = dataFolder+\"/airlineDataset/results\" #points to the results folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-1- Read csv file and/or convert to dataframe - Airline Dataset\n",
    "airline_DF_Clean = PAD.preprocessAirline(convertcsv = True, save = False, clean = True, \n",
    "                                         dataSource = airlineRawData, \n",
    "                                         dataStorage = airlineProcessedData)\n",
    "#airline_DF_Clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The topics captured in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-2- Get only topic labelled tweets - This is for exploration only\n",
    "tweets_Labelled = dp.getTopicLabelledTweets(dataSource=airlineProcessedData+'/airline_DF_Clean.pkl', \n",
    "                                              exclude = [\"Can't Tell\"], save = False)\n",
    "#Following lets us see what topics exists\n",
    "topicList = set(list(tweets_Labelled.negativeReason))\n",
    "topicList\n",
    "#Following gives me a count on the number of tweets we are dealing with\n",
    "#tweets_Labelled.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following allows you to access the stored preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetID</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>publicationDate</th>\n",
       "      <th>negativeReason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.703060e+17</td>\n",
       "      <td>say</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>2/24/15 11:35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>plus youve add commercial experience tacky</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>didnt today must mean need take another trip</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>2/24/15 11:15</td>\n",
       "      <td>Bad Flight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.703010e+17</td>\n",
       "      <td>really big bad thing</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>2/24/15 11:14</td>\n",
       "      <td>Can't Tell</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        tweetID                                               text  \\\n",
       "0  5.703060e+17                                                say   \n",
       "1  5.703010e+17         plus youve add commercial experience tacky   \n",
       "2  5.703010e+17       didnt today must mean need take another trip   \n",
       "3  5.703010e+17  really aggressive blast obnoxious entertainmen...   \n",
       "4  5.703010e+17                               really big bad thing   \n",
       "\n",
       "       author publicationDate negativeReason  \n",
       "0     cairdin   2/24/15 11:35            NaN  \n",
       "1    jnardino   2/24/15 11:15            NaN  \n",
       "2  yvonnalynn   2/24/15 11:15            NaN  \n",
       "3    jnardino   2/24/15 11:15     Bad Flight  \n",
       "4    jnardino   2/24/15 11:14     Can't Tell  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_DF_Clean = dp.getData(airlineProcessedData+'/airline_DF_Clean.pkl')\n",
    "airline_DF_Clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3. Data Tranformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We extract the corpus from our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = airline_DF_Clean['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now demonstrate our transformation algorithm.\n",
    "Please note that we only transform a fraction of the corpus for the sake of time. The fully transformed document set have already been completed and is available (see next step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the corpus as a slice of the tweets - feel free to change this \n",
    "corpus = tweets[:20]\n",
    "# the noOfBuckets is the number of documents that will be returned. \n",
    "# Here we want the number of documents returned to be the same as the \n",
    "# original number of documents \n",
    "noOfBuckets = len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 50/1018 [00:00<00:01, 499.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total initial cost is 1037.9989337433067\n",
      "The number of folds to perform is  1018\n",
      "The number documents is  1038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1018/1018 [00:00<00:00, 2487.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new total cost after round x is  19.999762106972675 and new corpus size is  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# the actual transformation\n",
    "transformed_docs = mdl.malg(corpus, noOfBuckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformed_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fully tranformed documents\n",
    "Get access to the saved fully transformed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fully_transfromed_docs = './data/airlineDataset/interim/transformedCorpus_Noisy.pkl'\n",
    "\n",
    "with open(fully_transfromed_docs, 'rb') as f:\n",
    "    transformed_data = pickle.load(f)\n",
    "    \n",
    "# transformed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modelling Approaches\n",
    "### (used for both transformed and untransformed data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a) LDA Topics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features too low, do not use 'n_features = auto' option,        specify a number\n",
      "[['reserve', 'away', 'seat', 'flight'], ['seat', 'reserve', 'flight', 'away'], ['seat', 'flight', 'reserve', 'away'], ['seat', 'flight', 'reserve', 'away'], ['flight', 'seat', 'reserve', 'away'], ['seat', 'flight', 'reserve', 'away'], ['seat', 'away', 'flight', 'reserve'], ['seat', 'flight', 'reserve', 'away'], ['seat', 'flight', 'reserve', 'away']]\n"
     ]
    }
   ],
   "source": [
    "foundTopics = mdl.getLDATopics(transformed_docs, n_topics= 9)\n",
    "print(foundTopics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) BTM Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def basic_evaluation(data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(data).toarray()\n",
    "    vocab = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "    biterms = vec_to_biterms(X)\n",
    "    # create btm\n",
    "    btm = oBTM(num_topics=9, V=vocab)\n",
    "\n",
    "    print(\"\\n\\n Train BTM ..\")\n",
    "    topics = btm.fit_transform(biterms, iterations=100)\n",
    "    return topic_summuary(btm.phi_wz.T, X, vocab, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# btm_airline_res = basic_evaluation(transformed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c) PTM Topics\n",
    "The three functions below are required to\n",
    "* generate corpus for the PTM model\n",
    "* run the PTM model\n",
    "* get the topics from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "\n",
    "def getCorpus(input_data):\n",
    "    corpus = tp.utils.Corpus(tokenizer=tp.utils.SimpleTokenizer(), stopwords=['.'])\n",
    "    corpus.process(input_data)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PTM_model (input_corpus, num_topics, a=None, b=None):\n",
    "    if a == None:\n",
    "        a = 1/num_topics\n",
    "    if b == None: \n",
    "        b = 1/num_topics\n",
    "    ptm_model = tp.PTModel(k=num_topics, alpha=a, eta=b, corpus=input_corpus)\n",
    "    for i in range(0, 1000, 10):\n",
    "        ptm_model.train(10)\n",
    "    return ptm_model\n",
    "\n",
    "def get_mdl_topics(mdl, k):\n",
    "    k_topics = []\n",
    "    for k in range(k):\n",
    "        word_set = mdl.get_topic_words(k, top_n=10)\n",
    "        k_topics.append([w[0]for w in word_set])\n",
    "    return k_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptm_corpus = getCorpus(transformed_data)\n",
    "ptm_mdl = PTM_model(ptm_corpus, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptm_topics = get_mdl_topics(ptm_mdl, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing discovered topics with true topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RecallActual': 0.8888888888888888,\n",
       " 'Precision': 0.8888888888888888,\n",
       " 'Purity': 0.35555555555555557}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('data/airlineDataset/results/gsrTopics.txt', 'r')\n",
    "d = {}\n",
    "for line in f:\n",
    "    (key, val) = line.split(':')\n",
    "    d[(key)] = list(val.strip().split(', '))\n",
    "f.close()\n",
    "ActualTopics = d\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# BTM results must be transformed to lists\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# topics = []\n",
    "# for e in btm_results['topic_summary']['top_words']:\n",
    "#     btm_topics.append(list(e))\n",
    "# btm_topics\n",
    "\n",
    "ev.evaluate_correctness(ptm_topics, ActualTopics, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
